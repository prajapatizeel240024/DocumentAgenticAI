{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 0 env loading (for GROQ_API_KEY, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load environment variables, including GROQ_API_KEY\n",
    "\n",
    "os.environ['JAVA_HOME'] = r\"C:\\Program Files\\Java\\jdk-21\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jpype\n",
    "import tabula\n",
    "import base64\n",
    "import pymupdf\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Summarization w/ GROQ\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Summarization w/ GPT-4o (OpenAI style)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# For VectorStore & RAG\n",
    "import uuid\n",
    "from base64 import b64decode\n",
    "from langchain_chroma import Chroma  # Updated import\n",
    "from langchain_openai import OpenAIEmbeddings  # Updated import\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JVM loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Start JVM for Tabula if not running\n",
    "jvm_path = r\"C:\\Program Files\\Java\\jdk-21\\bin\\server\\jvm.dll\"\n",
    "if not jpype.isJVMStarted():\n",
    "    jpype.startJVM(jvm_path)\n",
    "\n",
    "if jpype.isJVMStarted():\n",
    "    print(\"JVM loaded successfully!\")\n",
    "else:\n",
    "    print(\"JVM not loaded.\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PDF Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Path: data\\attention.pdf\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"data\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "filename = \"attention.pdf\"\n",
    "filepath = os.path.join(base_dir, filename)\n",
    "print(\"PDF Path:\", filepath)\n",
    "\n",
    "def create_directories(base_dir):\n",
    "    directories = [\"images\", \"text\", \"tables\", \"page_images\"]\n",
    "    for d in directories:\n",
    "        os.makedirs(os.path.join(base_dir, d), exist_ok=True)\n",
    "\n",
    "create_directories(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tables(doc, page_num, base_dir, items):\n",
    "    \"\"\"Extract tables with Tabula and save them as .txt files.\"\"\"\n",
    "    try:\n",
    "        tables = tabula.read_pdf(filepath, pages=page_num + 1, multiple_tables=True)\n",
    "        if not tables:\n",
    "            return\n",
    "        for table_idx, table in enumerate(tables):\n",
    "            table_text = \"\\n\".join([\" | \".join(map(str, row)) for row in table.values])\n",
    "            table_file_name = f\"{base_dir}/tables/{os.path.basename(filepath)}_table_{page_num}_{table_idx}.txt\"\n",
    "            with open(table_file_name, 'w', encoding='utf-8') as f:\n",
    "                f.write(table_text)\n",
    "            items.append({\"page\": page_num, \"type\": \"table\", \"text\": table_text, \"path\": table_file_name})\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables from page {page_num}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveCharacterTextSplitter:\n",
    "    \"\"\"A simple text splitter that chunks text by character length.\"\"\"\n",
    "    def __init__(self, chunk_size=700, chunk_overlap=200, length_function=len):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.length_function = length_function\n",
    "\n",
    "    def split_text(self, text):\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + self.chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            start += (self.chunk_size - self.chunk_overlap)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_chunks(text, text_splitter, page_num, base_dir, items):\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        text_file_name = f\"{base_dir}/text/{os.path.basename(filepath)}_text_{page_num}_{i}.txt\"\n",
    "        with open(text_file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(chunk)\n",
    "        items.append({\"page\": page_num, \"type\": \"text\", \"text\": chunk, \"path\": text_file_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(page, page_num, base_dir, items, doc):\n",
    "    \"\"\"Extract embedded images on a PDF page.\"\"\"\n",
    "    images = page.get_images()\n",
    "    for idx, image in enumerate(images):\n",
    "        xref = image[0]\n",
    "        pix = pymupdf.Pixmap(doc, xref)\n",
    "        image_name = f\"{base_dir}/images/{os.path.basename(filepath)}_image_{page_num}_{idx}_{xref}.png\"\n",
    "        pix.save(image_name)\n",
    "        with open(image_name, 'rb') as f:\n",
    "            encoded_image = base64.b64encode(f.read()).decode('utf8')\n",
    "        items.append({\"page\": page_num, \"type\": \"image\", \"path\": image_name, \"image\": encoded_image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page_images(page, page_num, base_dir, items):\n",
    "    \"\"\"Render the entire page as an image and store it.\"\"\"\n",
    "    pix = page.get_pixmap()\n",
    "    page_path = os.path.join(base_dir, f\"page_images/page_{page_num:03d}.png\")\n",
    "    pix.save(page_path)\n",
    "    with open(page_path, 'rb') as f:\n",
    "        page_image = base64.b64encode(f.read()).decode('utf8')\n",
    "    items.append({\"page\": page_num, \"type\": \"page\", \"path\": page_path, \"image\": page_image})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Partition the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m items \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal pages: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_pages\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_num \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_pages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessing PDF pages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     10\u001b[0m     page \u001b[38;5;241m=\u001b[39m doc[page_num]\n\u001b[0;32m     11\u001b[0m     text \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mget_text()\n",
      "File \u001b[1;32md:\\MultiModel\\venv\\lib\\site-packages\\tqdm\\std.py:665\u001b[0m, in \u001b[0;36mtqdm.__new__\u001b[1;34m(cls, *_, **__)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m__):\n\u001b[0;32m    664\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m--> 665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# also constructs lock if non-existent\u001b[39;00m\n\u001b[0;32m    666\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instances\u001b[38;5;241m.\u001b[39madd(instance)\n\u001b[0;32m    667\u001b[0m         \u001b[38;5;66;03m# create monitoring thread\u001b[39;00m\n",
      "File \u001b[1;32md:\\MultiModel\\venv\\lib\\site-packages\\tqdm\\std.py:764\u001b[0m, in \u001b[0;36mtqdm.get_lock\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the global lock. Construct it if it does not exist.\"\"\"\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 764\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_lock \u001b[38;5;241m=\u001b[39m \u001b[43mTqdmDefaultWriteLock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\n",
      "File \u001b[1;32md:\\MultiModel\\venv\\lib\\site-packages\\tqdm\\std.py:97\u001b[0m, in \u001b[0;36mTqdmDefaultWriteLock.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m root_lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     root_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_mp_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocks \u001b[38;5;241m=\u001b[39m [lk \u001b[38;5;28;01mfor\u001b[39;00m lk \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmp_lock, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mth_lock] \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m root_lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\MultiModel\\venv\\lib\\site-packages\\tqdm\\std.py:121\u001b[0m, in \u001b[0;36mTqdmDefaultWriteLock.create_mp_lock\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RLock\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmp_lock \u001b[38;5;241m=\u001b[39m \u001b[43mRLock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmp_lock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:72\u001b[0m, in \u001b[0;36mBaseContext.RLock\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mRLock\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Returns a recursive lock object'''\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msynchronize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RLock\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RLock(ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_context())\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1430\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1402\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1535\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "doc = pymupdf.open(filepath)\n",
    "num_pages = len(doc)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200)\n",
    "items = []\n",
    "\n",
    "print(f\"Total pages: {num_pages}\")\n",
    "\n",
    "for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text()\n",
    "\n",
    "    # Table extraction\n",
    "    process_tables(doc, page_num, base_dir, items)\n",
    "    # Text chunking\n",
    "    process_text_chunks(text, text_splitter, page_num, base_dir, items)\n",
    "    # Images\n",
    "    process_images(page, page_num, base_dir, items, doc)\n",
    "    # Full-page images\n",
    "    process_page_images(page, page_num, base_dir, items)\n",
    "\n",
    "print(\"\\nPartitioning complete!\")\n",
    "text_items = [i for i in items if i['type'] == 'text']\n",
    "table_items = [i for i in items if i['type'] == 'table']\n",
    "image_items = [i for i in items if i['type'] == 'image']\n",
    "\n",
    "if text_items:\n",
    "    print(\"Sample text item:\", text_items[0])\n",
    "if table_items:\n",
    "    print(\"Sample table item:\", table_items[0])\n",
    "if image_items:\n",
    "    print(\"Sample image item:\", image_items[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gather Text & Table Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text chunks: 123\n",
      "Number of table chunks: 2\n"
     ]
    }
   ],
   "source": [
    "text_dir = os.path.join(base_dir, \"text\")\n",
    "table_dir = os.path.join(base_dir, \"tables\")\n",
    "\n",
    "text_files = [f for f in os.listdir(text_dir) if f.endswith(\".txt\")]\n",
    "table_files = [f for f in os.listdir(table_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "text_chunks = []\n",
    "for filename in text_files:\n",
    "    fp = os.path.join(text_dir, filename)\n",
    "    with open(fp, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    text_chunks.append(content)\n",
    "\n",
    "table_chunks = []\n",
    "for filename in table_files:\n",
    "    fp = os.path.join(table_dir, filename)\n",
    "    with open(fp, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    table_chunks.append(content)\n",
    "\n",
    "print(f\"\\nNumber of text chunks: {len(text_chunks)}\")\n",
    "print(f\"Number of table chunks: {len(table_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Summarize Text & Tables (Groq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE TEXT SUMMARIES ===\n",
      "Text Chunk #1 SUMMARY:\n",
      "This survey on Agentic AI for scientific discovery at ICLR 2025 highlights the progress, challenges, and future directions of AI systems that enable reasoning, planning, and autonomous decision-making in research automation, transforming tasks such as literature review, hypothesis generation, experimentation, and result analysis.\n",
      "---\n",
      "Text Chunk #2 SUMMARY:\n",
      "The table or text chunk discusses Agentic AI for scientific discovery, providing an overview of existing systems, tools, and recent progress in fields like chemistry, biology, and materials science, while addressing challenges and outlining future research directions.\n",
      "---\n",
      "Text Chunk #3 SUMMARY:\n",
      "The rapid advancements of Large Language Models have opened a new era in scientific discovery, enabling Agentic AI systems to automate complex research workflows with high autonomy.\n",
      "---\n",
      "\n",
      "=== EXAMPLE TABLE SUMMARIES ===\n",
      "Table Chunk #1 SUMMARY:\n",
      "Recent studies on AI applications were published in 2023 (Yuan et al., Zhang et al., Abbasian et al., Nottingham et al.) and 2024 (Buehler, Deng et al., Furuta et al., Kang & Xiong, Xin et al., Zhou et al.) in various fields including web scenarios, gaming environments, and healthcare.\n",
      "---\n",
      "Table Chunk #2 SUMMARY:\n",
      "Recent studies in general science and machine learning (2022-2025) have contributed to the development of agentic AI, with notable publications from Merchant et al. (2023), Yang et al. (2023b), and Li et al. (2024b), among others.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Respond only with the summary, no additional comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "\"\"\"\n",
    "\n",
    "groq_prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "groq_model = ChatGroq(temperature=0.5, model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "summarize_chain = (\n",
    "    {\"element\": lambda x: x}\n",
    "    | groq_prompt\n",
    "    | groq_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "text_summaries = summarize_chain.batch(text_chunks, {\"max_concurrency\": 3})\n",
    "table_summaries = summarize_chain.batch(table_chunks, {\"max_concurrency\": 3})\n",
    "\n",
    "print(\"\\n=== EXAMPLE TEXT SUMMARIES ===\")\n",
    "for idx, summary in enumerate(text_summaries[:3]):\n",
    "    print(f\"Text Chunk #{idx+1} SUMMARY:\\n{summary}\\n---\")\n",
    "\n",
    "print(\"\\n=== EXAMPLE TABLE SUMMARIES ===\")\n",
    "for idx, summary in enumerate(table_summaries[:3]):\n",
    "    print(f\"Table Chunk #{idx+1} SUMMARY:\\n{summary}\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summarize Images (GPT-4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE IMAGE SUMMARY ===\n",
      "The image represents a cyclic workflow diagram with six stages, depicting the process of research development. Each stage is represented by a circle containing an image, and these circles are connected with green arrows, illustrating the sequence of stages in the workflow.\n",
      "\n",
      "1. **Idea Generation & Literature Review**:\n",
      "   - Contains an image of books and a magnifying glass, representing research references and exploration of existing literature.\n",
      "   - A small robot character is present in the image.\n",
      "\n",
      "2. **Research Planning & Experiment Design**:\n",
      "   - Displays bar plots and a line graph, symbolizing data collection and analysis planning.\n",
      "   - A magnifying glass hovers over these graphs, indicating scrutinization or focus.\n",
      "   - Includes the robot character.\n",
      "\n",
      "3. **Data Preparation & Experiment Execution**:\n",
      "   - Shows laboratory equipment with a liquid being poured into a beaker, symbolizing experimentation and data collection.\n",
      "   - The robot character is included in the setup.\n",
      "\n",
      "4. **Results Analysis**:\n",
      "   - Features a computer screen displaying graphs and charts, representing data analysis.\n",
      "   - A person appears to be interacting with the computer, alongside the robot character.\n",
      "\n",
      "5. **Report Writing & Synthesis**:\n",
      "   - Depicts a person writing in a notebook, symbolizing the documentation and synthesis of research findings.\n",
      "   - The robot character is sitting on the notebook.\n",
      "\n",
      "6. **Paper Review**:\n",
      "   - Shows a person reviewing and writing on a printed document, likely a research paper.\n",
      "   - The robot character appears alongside the person.\n",
      "\n",
      "The green arrows circulate clockwise, indicating the continuity and iterative nature of the research process.\n"
     ]
    }
   ],
   "source": [
    "img_prompt_template = \"\"\"Describe the image in detail. For context,\n",
    "the image is part of a research paper explaining the transformers architecture.\n",
    "Be specific about graphs, such as bar plots.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": img_prompt_template},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "image_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "image_chain = image_prompt | ChatOpenAI(model=\"gpt-4o\") | StrOutputParser()\n",
    "\n",
    "image_dir = os.path.join(base_dir, \"images\")\n",
    "image_files = [f for f in os.listdir(image_dir) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "\n",
    "images_base64 = []\n",
    "for filename in image_files:\n",
    "    file_path = os.path.join(image_dir, filename)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "    images_base64.append(base64.b64encode(content).decode('utf8'))\n",
    "\n",
    "image_summaries = image_chain.batch(images_base64)\n",
    "if image_summaries:\n",
    "    print(\"\\n=== EXAMPLE IMAGE SUMMARY ===\")\n",
    "    print(image_summaries[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Vector Store & InMemoryStore Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# Define source data\n",
    "texts = text_chunks      # From earlier PDF processing\n",
    "tables = table_chunks    # From table extraction\n",
    "images = images_base64   # From image extraction\n",
    "\n",
    "# Make sure we have summaries\n",
    "if not 'text_summaries' in locals():\n",
    "    text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 3})\n",
    "if not 'table_summaries' in locals():\n",
    "    table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 3})\n",
    "if not 'image_summaries' in locals():\n",
    "    image_summaries = image_chain.batch(images)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multi_modal_rag\",\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Add Summaries + Link Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define your source data\n",
    "texts = text_chunks  # or however you're getting your text data\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 3})\n",
    "\n",
    "#  Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# Add image summaries\n",
    "img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "summary_img = [\n",
    "    Document(page_content=summary, metadata={id_key: img_ids[i]}) for i, summary in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "retriever.docstore.mset(list(zip(img_ids, images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hang Zeng, Zhang-Ren Chen, and Bowen Zhou.\n",
      "Large language models are zero shot hypothesis proposers. arXiv preprint arXiv:2311.05965,\n",
      "2023.\n",
      "Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen,\n",
      "Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. In Pro-\n",
      "ceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume\n",
      "1: Long Papers), pp. 15174–15186, 2024.\n",
      "Yixiang Ruan, Chenyin Lu, Ning Xu, Jian Zhang, Jun Xuan, Jianzhang Pan, Qun Fang, Hanyu Gao,\n",
      "Xiaodong Shen, Ning Ye, et al. Accelerated end-to-end chemical synthesis development with\n",
      "large language models. doi:10.26434/chemrxiv-2024-6wmg4, 20\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ction\n",
      "PubChem Kim et al. (2016)\n",
      "Chemistry\n",
      "Molecular feature extraction\n",
      "Mol-Instructions Fang et al.\n",
      "(2023)\n",
      "Biology/Chemistry\n",
      "Protein\n",
      "and\n",
      "biomolecular-\n",
      "related tasks\n",
      "MPcules Spotte-Smith et al.\n",
      "(2023)\n",
      "Materials Science\n",
      "Molecular properties\n",
      "AlphaFold\n",
      "Protein\n",
      "Struc-\n",
      "ture Varadi et al. (2022)\n",
      "Biology\n",
      "Protein structure prediction\n",
      "ICLR 2022 OpenReview Lu\n",
      "et al. (2024)\n",
      "Scientific Research\n",
      "Performance evaluation of the\n",
      "automated paper reviewer\n",
      "1https://www.letta.com/\n",
      "7\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      ".\n",
      "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\n",
      "man, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical\n",
      "report. arXiv preprint arXiv:2303.08774, 2023.\n",
      "Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, and Tianyu Gao.\n",
      "LitSearch: A retrieval benchmark for scientific literature search. In Yaser Al-Onaizan, Mohit\n",
      "Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Meth-\n",
      "ods in Natural Language Processing, pp. 15068–15083, Miami, Florida, USA, November 2024.\n",
      "Association for Computational Linguistics.\n",
      "doi: 10.18653/v1/2024.emnlp-main.840.\n",
      "URL\n",
      "https://aclanthology.org\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Published as a conference paper at ICLR 2025\n",
      "Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G Patil, Ion Stoica, and Joseph E\n",
      "Gonzalez. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560, 2023.\n",
      "Ioannis Papadimitriou, Ilias Gialampoukidis, Stefanos Vrochidis, and Ioannis Kompatsiaris. Ai\n",
      "methods in materials design, discovery and manufacturing: A review. Computational Materials\n",
      "Science, 235:112793, 2024.\n",
      "Ori Press, Andreas Hochlehnert, Ameya Prabhu, Vishaal Udandarao, Ofir Press, and Matthias\n",
      "Bethge.\n",
      "Citeme: Can language models accurately cite scientific claims?\n",
      "arXiv preprint\n",
      "arXiv:2407.12861, 2024.\n",
      "Michael H Prince, Henry Chan, Aikaterini Vriza, Tao Z\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Retrieve\n",
    "docs = retriever.invoke(\n",
    "    \"What this document is about?\"\n",
    ")\n",
    "for doc in docs:\n",
    "    print(str(doc) + \"\\n\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from base64 import b64decode\n",
    "from base64 import b64decode\n",
    "import imghdr\n",
    "\n",
    "def parse_docs(docs):\n",
    "    \"\"\"\n",
    "    Return {\"images\": [(b64, mime), ...], \"texts\": [str, ...]}\n",
    "    If base64‑decoding succeeds, we treat it as an image and\n",
    "    guess its mime ('png' / 'jpeg'); otherwise it's text.\n",
    "    \"\"\"\n",
    "    images, texts = [], []\n",
    "    for d in docs:\n",
    "        try:\n",
    "            raw = b64decode(d)\n",
    "        except Exception:\n",
    "            texts.append(d)                # not base64 → plain text\n",
    "            continue\n",
    "\n",
    "        # Guess the format from the first bytes\n",
    "        kind = imghdr.what(None, h=raw)    # 'png', 'jpeg', etc.\n",
    "        if kind in (\"png\", \"jpeg\", \"gif\", \"webp\"):\n",
    "            images.append((d, kind))\n",
    "        else:                              # something weird → treat as text\n",
    "            texts.append(d)\n",
    "    return {\"images\": images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "    ctx = kwargs[\"context\"]\n",
    "    question = kwargs[\"question\"]\n",
    "\n",
    "    # 2a. Add textual context\n",
    "    context_text = \"\\n\".join(ctx[\"texts\"])\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "Answer the question using **only** the following context (text, tables,\n",
    "and any images provided). If the images are irrelevant, ignore them.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "\n",
    "    # 2b. Attach images with the correct mime\n",
    "    for b64_str, mime in ctx[\"images\"]:\n",
    "        prompt_content.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/{mime};base64,{b64_str}\"}\n",
    "        })\n",
    "\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [HumanMessage(content=prompt_content)]\n",
    "    )\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt)\n",
    "    | ChatOpenAI(model=\"gpt-4o\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(\n",
    "    response=(\n",
    "        RunnableLambda(build_prompt)\n",
    "        | ChatOpenAI(model=\"gpt-4o\")\n",
    "        | StrOutputParser()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between single and multi-agent systems lies in their structure and application suitability:\n",
      "\n",
      "- **Single Agent Systems**: These involve an individual agent capable of achieving its goals independently without relying on assistance or feedback from other AI agents, even if multiple agents are present in the environment. They are ideal for well-defined problems where user feedback is not necessary. A single agent system can have an LLM backbone and is able to perform tasks such as reasoning, planning, and tool execution independently.\n",
      "\n",
      "- **Multi-Agent Systems**: These systems comprise two or more agents interacting with each other. They are inspired by the theory where smaller agents with specific functions interact to create intelligence. Multi-agent systems require interoperability for communication and information sharing and are suited for jobs that require collaboration across multiple domains, where each agent is an expert in a particular area. Multi-agent systems are particularly useful in scenarios where tasks are complex and span multiple domains.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"How many Allrgies they do have?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: show a base‑64 encoded image inline (works in Jupyter / VS Code)\n",
    "from IPython.display import Image, display\n",
    "import base64\n",
    "\n",
    "def display_base64_image(img):\n",
    "    \"\"\"\n",
    "    Accepts either a raw base64 string *or* a (b64, mime) tuple,\n",
    "    decodes it, and renders the image inline.\n",
    "    \"\"\"\n",
    "    if isinstance(img, tuple):         # the parser may return (b64, mime)\n",
    "        img_b64, _mime = img\n",
    "    else:\n",
    "        img_b64 = img\n",
    "\n",
    "    display(Image(data=base64.b64decode(img_b64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The AI agent frameworks for scientific discovery are categorized into autonomous and collaborative frameworks. These frameworks are involved in various domains including materials science, general science, and machine learning. They automate tasks such as hypothesis generation, experiment design, data analysis, and literature review. This automation helps accelerate scientific discoveries, reduce costs, and democratize access to research tools. The systems are designed to collaborate with researchers, generating novel ideas and handling repetitive tasks. AI agents are particularly impactful in biology, where they assist in areas like genomics, drug discovery, and synthetic biology, integrating with laboratory tools to enhance research accuracy and reproducibility.\n",
      "\n",
      "\n",
      "Context:\n"
     ]
    }
   ],
   "source": [
    "response = chain_with_sources.invoke(\n",
    "    \"What's there Surgical History?\"\n",
    ")\n",
    "\n",
    "print(\"Response:\", response['response'])\n",
    "\n",
    "print(\"\\n\\nContext:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
